---
license: cc0-1.0
task_categories:
  - text-classification
  - question-answering
  - text-generation
language:
  - en
tags:
  - llm
  - ai-safety
  - ethics
  - benchmarking
  - evaluation
  - alignment
  - decision-making
  - gpt-5
  - claude
  - gemini
  - grok
  - theory-action-gap
  - evaluation-deployment-gap
size_categories:
  - 10K<n<100K
pretty_name: When Agents Act - LLM Ethical Decision-Making
dataset_info:
  features:
    - name: judgement_id
      dtype: string
    - name: dilemma_id
      dtype: string
    - name: model_id
      dtype: string
    - name: mode
      dtype: string
    - name: choice_id
      dtype: string
    - name: confidence
      dtype: float64
    - name: perceived_difficulty
      dtype: float64
    - name: reasoning_preview
      dtype: string
    - name: variable_values_json
      dtype: string
    - name: variation_key
      dtype: string
  splits:
    - name: judgements
      num_examples: 12802
    - name: dilemmas
      num_examples: 20
  configs:
    - config_name: default
      data_files:
        - split: judgements
          path: judgements_flat.csv
        - split: dilemmas
          path: dilemmas_flat.csv
---

# Dataset Card for "When Agents Act"

## Dataset Description

- **Homepage:** https://values.md
- **Repository:** https://github.com/values-md/dilemmas-api
- **Paper:** See `findings.md` in this repository
- **Point of Contact:** George Strakhov

### Dataset Summary

This dataset contains 12,802 ethical decision judgements from four frontier LLMs (GPT-5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Grok-4) across 20 AI-relevant ethical dilemmas with 1,601 variable configurations. Models were tested in both theory mode (hypothetical reasoning) and action mode (tool-enabled agents believing actions would execute).

**Key Finding:** Models reverse their ethical decisions 33.4% of the time when transitioning from theory to action mode.

### Supported Tasks

- **Evaluation-Deployment Gap Research:** Study behavioral shifts between evaluation and production contexts
- **Model Comparison:** Compare ethical decision-making across frontier LLMs
- **Bias Detection:** Analyze demographic sensitivity in AI decisions
- **Benchmark Validity:** Study generator-judge calibration in LLM-generated benchmarks

### Languages

English (en)

## Dataset Structure

### Data Instances

A typical judgement instance:

```json
{
  "judgement_id": "abc123...",
  "dilemma_id": "def456...",
  "model_id": "openai/gpt-5",
  "mode": "theory",
  "choice_id": "report_immediately",
  "confidence": 8.5,
  "perceived_difficulty": 6.0,
  "reasoning_preview": "Given the potential harm...",
  "variable_values_json": "{"DOCTOR_NAME": "Dr. Smith"}",
  "variation_key": "a1b2c3d4"
}
```

### Data Fields

See `CODEBOOK.md` for complete field descriptions. Key fields:

- `model_id`: LLM identifier (gpt-5, claude-4.5-sonnet, gemini-2.5-pro, grok-4)
- `mode`: "theory" or "action"
- `choice_id`: Selected decision option
- `confidence`: Self-reported confidence (0-10)
- `perceived_difficulty`: Judge's difficulty perception (0-10)
- `variation_key`: Unique identifier for variable configuration

### Data Splits

- `judgements`: 12,802 decision records
- `dilemmas`: 20 ethical scenarios

## Dataset Creation

### Curation Rationale

This dataset was created to study the evaluation-deployment gap in LLMs - whether models behave differently when they believe actions have real consequences versus hypothetical reasoning. Understanding this gap is critical for AI safety as standard benchmarks may not predict production behavior.

### Source Data

#### Initial Data Collection and Normalization

Dilemmas were generated using Gemini 2.5 Flash with explicit difficulty targets (1-10). Each dilemma includes:
- Situation description with variable placeholders
- 2-4 discrete choice options
- Demographic/contextual variables for bias testing
- Scenario modifiers (time pressure, stakes, uncertainty)
- Tools for action mode

#### Who are the source language producers?

Dilemmas were generated by Gemini 2.5 Flash. Judgements were produced by GPT-5, Claude 4.5 Sonnet, Gemini 2.5 Pro, and Grok-4.

### Annotations

#### Annotation process

Models were presented with ethical dilemmas in two conditions:
- **Theory mode:** "What should be done?" (hypothetical reasoning)
- **Action mode:** Tool-enabled agent with belief that actions would execute

All judgements include:
- Choice selection
- Self-reported confidence (0-10)
- Perceived difficulty (0-10)
- Full reasoning trace

#### Who are the annotators?

The four frontier LLMs serve as both subjects and annotators (self-reported metrics).

### Personal and Sensitive Information

No personal information. All scenarios involve fictional characters and situations.

## Considerations for Using the Data

### Social Impact of Dataset

This dataset enables research on AI safety, evaluation methodology, and ethical decision-making in LLMs. Understanding the evaluation-deployment gap is critical for:
- Improving AI safety assurance methods
- Developing more valid benchmarks
- Informing model selection for production deployments

### Discussion of Biases

The dataset intentionally includes demographic variables to enable bias detection research. Observed biases reflect model behavior and should not be interpreted as ground truth for ethical decisions.

### Other Known Limitations

- Single temperature setting (1.0)
- English language only
- Limited to 4 frontier models
- No human baseline for comparison

## Additional Information

### Dataset Curators

Claude (Anthropic) and George Strakhov (Independent Researcher)

### Licensing Information

CC0 1.0 Universal (Public Domain Dedication)

### Citation Information

```bibtex
@dataset{when_agents_act_2025,
  title={When Agents Act: Behavioral Shifts in Large Language Model Ethical Decision-Making from Evaluation to Deployment},
  author={Claude (Anthropic) and Strakhov, George},
  year={2025},
  month={November},
  publisher={Hugging Face},
  url={https://huggingface.co/datasets/...}
}
```

### Contributions

Research conducted using Claude (Anthropic) under the direction of George Strakhov.
